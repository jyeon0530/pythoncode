{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.90357846\n",
      "2000 0.52571344\n",
      "4000 0.5010264\n",
      "6000 0.48950568\n",
      "8000 0.48338956\n",
      "10000 0.4798303\n",
      "12000 0.47761104\n",
      "14000 0.47614932\n",
      "16000 0.47514245\n",
      "18000 0.47442222\n",
      "20000 0.4738901\n",
      "22000 0.473486\n",
      "24000 0.4731724\n",
      "26000 0.47292385\n",
      "28000 0.47272396\n",
      "30000 0.47256097\n",
      "32000 0.4724266\n",
      "34000 0.47231472\n",
      "36000 0.47222114\n",
      "38000 0.47214228\n",
      "40000 0.47207558\n",
      "42000 0.47201863\n",
      "44000 0.47197023\n",
      "46000 0.4719289\n",
      "48000 0.47189343\n",
      "50000 0.47186303\n",
      "52000 0.4718367\n",
      "54000 0.4718142\n",
      "56000 0.47179475\n",
      "58000 0.47177804\n",
      "60000 0.47176367\n",
      "62000 0.47175118\n",
      "64000 0.47174037\n",
      "66000 0.47173113\n",
      "68000 0.4717231\n",
      "70000 0.471716\n",
      "72000 0.47171\n",
      "74000 0.47170487\n",
      "76000 0.47170028\n",
      "78000 0.47169626\n",
      "80000 0.47169292\n",
      "82000 0.47168997\n",
      "84000 0.47168744\n",
      "86000 0.47168514\n",
      "88000 0.47168323\n",
      "90000 0.47168162\n",
      "92000 0.47168022\n",
      "94000 0.4716789\n",
      "96000 0.47167793\n",
      "98000 0.4716769\n",
      "100000 0.47167614\n",
      "Accuracy: 0.76943344\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "xy = np.loadtxt(r'C:\\Users\\user\\sample_repo\\diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:,0:-1]\n",
    "y_data = xy[:,[-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[None,8])#두개의 값\n",
    "Y = tf.placeholder(tf.float32, shape=[None,1])# 하나의 값\n",
    "\n",
    "\n",
    "# a1x1+a2x2로 빠뀜\n",
    "# a1x1+a2x2는 행렬곱을 이용해서 [a1,a]*[x1,x2]로 표현 가능\n",
    "# matmul()를 이용해 행렬곱을 적용\n",
    "# 시그모이드를 계산하기 위해 텐서플로에 내장된 sigmodi() 함수를 사용\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.random_normal([8,1]), name ='weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name ='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost) \n",
    "                \n",
    "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)#cast메소드란? True/False를 반환하는 코드\n",
    "#시그모이그 값에서 0과 1로 캐스팅한 값이 들어가 있음\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype=tf.float32))       \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())# 변수 초기화\n",
    "\n",
    "    for step in range(100001):\n",
    "        cost_val, _ = sess.run([cost,train], feed_dict= {X:x_data, Y: y_data})\n",
    "        \n",
    "        if step % 2000 == 0:\n",
    "                print(step, cost_val)\n",
    "   \n",
    "    _,_,a = sess.run([hypothesis, predicted, accuracy], feed_dict ={X:x_data, Y: y_data})\n",
    "    print(\"Accuracy:\",a)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀에서 퍼센트론으로"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "퍼센트론: 입력 값과 활성화 함수를 사용해 출력 값을 다음으로 넘기는 가장 작은 신경망 단위\n",
    "y = ax+b(a는 기울기, b는 y절편)\n",
    "-> y = wa+b(w:가중치, b는 바이어스)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 cost: 0.8235795 W= [[-0.70767856]\n",
      " [ 0.24036685]] b = [0.43324077]\n",
      "step =  100 cost: 0.5104901 W= [[0.04381768]\n",
      " [0.5725982 ]] b = [-1.0054199]\n",
      "step =  200 cost: 0.3896171 W= [[0.72290194]\n",
      " [1.0313562 ]] b = [-1.7332456]\n",
      "step =  300 cost: 0.31853172 W= [[1.21606  ]\n",
      " [1.4015044]] b = [-2.3061802]\n",
      "step =  400 cost: 0.2712457 W= [[1.5984458]\n",
      " [1.7134192]] b = [-2.7841763]\n",
      "step =  500 cost: 0.23704186 W= [[1.9117376]\n",
      " [1.9851578]] b = [-3.1960974]\n",
      "step =  600 cost: 0.21087602 W= [[2.1785905]\n",
      " [2.226782 ]] b = [-3.5592687]\n",
      "step =  700 cost: 0.19006835 W= [[2.412181]\n",
      " [2.444618]] b = [-3.8848026]\n",
      "step =  800 cost: 0.1730512 W= [[2.620692 ]\n",
      " [2.6430302]] b = [-4.180234]\n",
      "step =  900 cost: 0.1588363 W= [[2.8095129]\n",
      " [2.8252218]] b = [-4.45093]\n",
      "step =  1000 cost: 0.14676376 W= [[2.9823759]\n",
      " [2.9936323]] b = [-4.7008653]\n",
      "step =  1100 cost: 0.13637266 W= [[3.1419766]\n",
      " [3.150184 ]] b = [-4.933079]\n",
      "step =  1200 cost: 0.12732926 W= [[3.2903347]\n",
      " [3.2964134]] b = [-5.1499577]\n",
      "step =  1300 cost: 0.11938481 W= [[3.429012]\n",
      " [3.43358 ]] b = [-5.353421]\n",
      "step =  1400 cost: 0.112349406 W= [[3.559243 ]\n",
      " [3.5627227]] b = [-5.5450363]\n",
      "step =  1500 cost: 0.10607539 W= [[3.6820276]\n",
      " [3.68471  ]] b = [-5.7261]\n",
      "step =  1600 cost: 0.10044579 W= [[3.7981873]\n",
      " [3.8002791]] b = [-5.897708]\n",
      "step =  1700 cost: 0.0953666 W= [[3.9084094]\n",
      " [3.910058 ]] b = [-6.060784]\n",
      "step =  1800 cost: 0.090761386 W= [[4.0132732]\n",
      " [4.014586 ]] b = [-6.2161264]\n",
      "step =  1900 cost: 0.086567305 W= [[4.1132793]\n",
      " [4.1143327]] b = [-6.3644233]\n",
      "step =  2000 cost: 0.08273226 W= [[4.2088547]\n",
      " [4.2097073]] b = [-6.506274]\n",
      "step =  2100 cost: 0.07921251 W= [[4.300371]\n",
      " [4.301067]] b = [-6.642206]\n",
      "step =  2200 cost: 0.075971134 W= [[4.3881583]\n",
      " [4.388731 ]] b = [-6.7726836]\n",
      "step =  2300 cost: 0.072976775 W= [[4.4725037]\n",
      " [4.4729767]] b = [-6.8981223]\n",
      "step =  2400 cost: 0.07020267 W= [[4.5536647]\n",
      " [4.554059 ]] b = [-7.0188775]\n",
      "step =  2500 cost: 0.067625664 W= [[4.631868 ]\n",
      " [4.6321974]] b = [-7.135285]\n",
      "step =  2600 cost: 0.06522572 W= [[4.707317 ]\n",
      " [4.7075944]] b = [-7.24764]\n",
      "step =  2700 cost: 0.06298552 W= [[4.7801957]\n",
      " [4.780432 ]] b = [-7.3562074]\n",
      "step =  2800 cost: 0.06088978 W= [[4.850671]\n",
      " [4.850875]] b = [-7.461231]\n",
      "step =  2900 cost: 0.058925226 W= [[4.9188952]\n",
      " [4.9190702]] b = [-7.562927]\n",
      "step =  3000 cost: 0.05708003 W= [[4.985004 ]\n",
      " [4.9851537]] b = [-7.661496]\n",
      "step =  3100 cost: 0.05534382 W= [[5.049122]\n",
      " [5.04925 ]] b = [-7.75712]\n",
      "step =  3200 cost: 0.053707354 W= [[5.111363 ]\n",
      " [5.1114736]] b = [-7.8499656]\n",
      "step =  3300 cost: 0.05216235 W= [[5.17183 ]\n",
      " [5.171927]] b = [-7.940185]\n",
      "step =  3400 cost: 0.05070155 W= [[5.2306237]\n",
      " [5.2307067]] b = [-8.0279255]\n",
      "step =  3500 cost: 0.049318288 W= [[5.2878294]\n",
      " [5.2879004]] b = [-8.113308]\n",
      "step =  3600 cost: 0.0480067 W= [[5.343527]\n",
      " [5.34359 ]] b = [-8.196456]\n",
      "step =  3700 cost: 0.046761308 W= [[5.3977942]\n",
      " [5.39785  ]] b = [-8.277482]\n",
      "step =  3800 cost: 0.045577444 W= [[5.4507017]\n",
      " [5.450751 ]] b = [-8.356489]\n",
      "step =  3900 cost: 0.04445061 W= [[5.502314]\n",
      " [5.502358]] b = [-8.433574]\n",
      "step =  4000 cost: 0.0433769 W= [[5.552694]\n",
      " [5.55273 ]] b = [-8.508823]\n",
      "step =  4100 cost: 0.042352654 W= [[5.6018944]\n",
      " [5.6019273]] b = [-8.582323]\n",
      "step =  4200 cost: 0.041374568 W= [[5.64997 ]\n",
      " [5.649999]] b = [-8.65415]\n",
      "step =  4300 cost: 0.04043966 W= [[5.6969705]\n",
      " [5.6969957]] b = [-8.724378]\n",
      "step =  4400 cost: 0.03954513 W= [[5.7429414]\n",
      " [5.742963 ]] b = [-8.793075]\n",
      "step =  4500 cost: 0.03868854 W= [[5.7879243]\n",
      " [5.787945 ]] b = [-8.860305]\n",
      "step =  4600 cost: 0.037867468 W= [[5.8319626]\n",
      " [5.8319817]] b = [-8.926128]\n",
      "step =  4700 cost: 0.037079815 W= [[5.8750944]\n",
      " [5.8751116]] b = [-8.9906]\n",
      "step =  4800 cost: 0.036323603 W= [[5.917354 ]\n",
      " [5.9173703]] b = [-9.053774]\n",
      "step =  4900 cost: 0.035597004 W= [[5.9587765]\n",
      " [5.9587913]] b = [-9.115701]\n",
      "step =  5000 cost: 0.034898333 W= [[5.9993954]\n",
      " [5.999406 ]] b = [-9.17643]\n",
      "step =  5100 cost: 0.034226075 W= [[6.0392365]\n",
      " [6.0392456]] b = [-9.236002]\n",
      "step =  5200 cost: 0.033578653 W= [[6.0783305]\n",
      " [6.078339 ]] b = [-9.294461]\n",
      "step =  5300 cost: 0.032954805 W= [[6.1167045]\n",
      " [6.1167126]] b = [-9.351849]\n",
      "step =  5400 cost: 0.03235337 W= [[6.1543846]\n",
      " [6.1543927]] b = [-9.408201]\n",
      "step =  5500 cost: 0.03177304 W= [[6.1913943]\n",
      " [6.191402 ]] b = [-9.463555]\n",
      "step =  5600 cost: 0.031212775 W= [[6.227758]\n",
      " [6.227764]] b = [-9.517945]\n",
      "step =  5700 cost: 0.030671634 W= [[6.2634964]\n",
      " [6.2635016]] b = [-9.571403]\n",
      "step =  5800 cost: 0.030148536 W= [[6.2986293]\n",
      " [6.298634 ]] b = [-9.62396]\n",
      "step =  5900 cost: 0.029642737 W= [[6.333179 ]\n",
      " [6.3331833]] b = [-9.675645]\n",
      "step =  6000 cost: 0.02915334 W= [[6.3671618]\n",
      " [6.367166 ]] b = [-9.726484]\n",
      "step =  6100 cost: 0.028679503 W= [[6.400598]\n",
      " [6.400602]] b = [-9.776507]\n",
      "step =  6200 cost: 0.028220642 W= [[6.433503]\n",
      " [6.433507]] b = [-9.825739]\n",
      "step =  6300 cost: 0.027775997 W= [[6.4658933]\n",
      " [6.465897 ]] b = [-9.874204]\n",
      "step =  6400 cost: 0.027344823 W= [[6.4977846]\n",
      " [6.4977884]] b = [-9.921924]\n",
      "step =  6500 cost: 0.026926666 W= [[6.5291934]\n",
      " [6.5291953]] b = [-9.968921]\n",
      "step =  6600 cost: 0.02652093 W= [[6.5601306]\n",
      " [6.560133 ]] b = [-10.015218]\n",
      "step =  6700 cost: 0.026127078 W= [[6.590613 ]\n",
      " [6.5906153]] b = [-10.060834]\n",
      "step =  6800 cost: 0.025744505 W= [[6.6206517]\n",
      " [6.620654 ]] b = [-10.105789]\n",
      "step =  6900 cost: 0.025372839 W= [[6.65026  ]\n",
      " [6.6502624]] b = [-10.150102]\n",
      "step =  7000 cost: 0.0250116 W= [[6.6794486]\n",
      " [6.679451 ]] b = [-10.193789]\n",
      "step =  7100 cost: 0.024660341 W= [[6.708232]\n",
      " [6.708234]] b = [-10.23687]\n",
      "step =  7200 cost: 0.024318665 W= [[6.7366185]\n",
      " [6.7366204]] b = [-10.27936]\n",
      "step =  7300 cost: 0.02398616 W= [[6.7646203]\n",
      " [6.764622 ]] b = [-10.321274]\n",
      "step =  7400 cost: 0.02366253 W= [[6.7922473]\n",
      " [6.792249 ]] b = [-10.362626]\n",
      "step =  7500 cost: 0.023347396 W= [[6.819508 ]\n",
      " [6.8195095]] b = [-10.403433]\n",
      "step =  7600 cost: 0.023040397 W= [[6.8464136]\n",
      " [6.846415 ]] b = [-10.44371]\n",
      "step =  7700 cost: 0.02274129 W= [[6.872972]\n",
      " [6.872973]] b = [-10.483467]\n",
      "step =  7800 cost: 0.022449728 W= [[6.8991923]\n",
      " [6.8991933]] b = [-10.522719]\n",
      "step =  7900 cost: 0.022165427 W= [[6.9250827]\n",
      " [6.9250836]] b = [-10.561479]\n",
      "step =  8000 cost: 0.021888172 W= [[6.9506516]\n",
      " [6.9506526]] b = [-10.599759]\n",
      "step =  8100 cost: 0.021617686 W= [[6.9759064]\n",
      " [6.9759073]] b = [-10.637569]\n",
      "step =  8200 cost: 0.021353668 W= [[7.0008545]\n",
      " [7.0008554]] b = [-10.674922]\n",
      "step =  8300 cost: 0.02109597 W= [[7.0255036]\n",
      " [7.0255046]] b = [-10.711827]\n",
      "step =  8400 cost: 0.020844331 W= [[7.04986 ]\n",
      " [7.049861]] b = [-10.748296]\n",
      "step =  8500 cost: 0.020598572 W= [[7.0739307]\n",
      " [7.0739317]] b = [-10.784338]\n",
      "step =  8600 cost: 0.020358454 W= [[7.097723]\n",
      " [7.097724]] b = [-10.819964]\n",
      "step =  8700 cost: 0.020123795 W= [[7.121244]\n",
      " [7.121245]] b = [-10.855181]\n",
      "step =  8800 cost: 0.019894388 W= [[7.144497]\n",
      " [7.144498]] b = [-10.889999]\n",
      "step =  8900 cost: 0.019670162 W= [[7.1674905]\n",
      " [7.1674914]] b = [-10.92443]\n",
      "step =  9000 cost: 0.019450814 W= [[7.1902275]\n",
      " [7.1902285]] b = [-10.958479]\n",
      "step =  9100 cost: 0.01923624 W= [[7.212716 ]\n",
      " [7.2127166]] b = [-10.992156]\n",
      "step =  9200 cost: 0.01902629 W= [[7.234961 ]\n",
      " [7.2349615]] b = [-11.02547]\n",
      "step =  9300 cost: 0.01882088 W= [[7.256967 ]\n",
      " [7.2569675]] b = [-11.058424]\n",
      "step =  9400 cost: 0.018619793 W= [[7.278739 ]\n",
      " [7.2787395]] b = [-11.091029]\n",
      "step =  9500 cost: 0.018422857 W= [[7.3002825]\n",
      " [7.300283 ]] b = [-11.123295]\n",
      "step =  9600 cost: 0.018230021 W= [[7.321601 ]\n",
      " [7.3216014]] b = [-11.155219]\n",
      "step =  9700 cost: 0.018041177 W= [[7.3427   ]\n",
      " [7.3427005]] b = [-11.186821]\n",
      "step =  9800 cost: 0.01785611 W= [[7.363584 ]\n",
      " [7.3635845]] b = [-11.218098]\n",
      "step =  9900 cost: 0.017674781 W= [[7.3842573]\n",
      " [7.384258 ]] b = [-11.249059]\n",
      "step =  10000 cost: 0.01749704 W= [[7.4047227]\n",
      " [7.404723 ]] b = [-11.279713]\n",
      "\n",
      "Hypothesis: [[1.2576580e-05]\n",
      " [2.0332575e-02]\n",
      " [2.0332545e-02]\n",
      " [9.7152203e-01]] \n",
      "Correct: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]] \\mAccuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[0],[0],[1]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2], name= 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None,1], name= 'y-input')\n",
    "\n",
    "w = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,w)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype= tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step %100 ==0:\n",
    "            print(\"step = \", step, \"cost:\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W=\", sess.run(w), \"b =\", sess.run(b))\n",
    "            \n",
    "#accuacy\n",
    "    h, c, a = sess.run([hypothesis, predicted, accuracy],feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\nHypothesis:',h,\"\\nCorrect:\",c, \"\\mAccuracy:\",a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 퍼센트론의 과제\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  \n",
    "exclusiveOR(XOR) 두개으면 0 다르면 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  0 cost: 0.7909563 W= [[-1.6263758]\n",
      " [-0.7919976]] b = [1.1269284]\n",
      "step =  100 cost: 0.73445684 W= [[-0.9915516 ]\n",
      " [-0.54258645]] b = [0.9067746]\n",
      "step =  200 cost: 0.7122133 W= [[-0.64454365]\n",
      " [-0.4037949 ]] b = [0.6231574]\n",
      "step =  300 cost: 0.70185506 W= [[-0.42085832]\n",
      " [-0.2920539 ]] b = [0.42327952]\n",
      "step =  400 cost: 0.6971081 W= [[-0.2758694 ]\n",
      " [-0.20701812]] b = [0.28652903]\n",
      "step =  500 cost: 0.694947 W= [[-0.18162489]\n",
      " [-0.1448342 ]] b = [0.1936563]\n",
      "step =  600 cost: 0.69396526 W= [[-0.12008356]\n",
      " [-0.10042735]] b = [0.13079154]\n",
      "step =  700 cost: 0.69351923 W= [[-0.07969411]\n",
      " [-0.06919296]] b = [0.08830448]\n",
      "step =  800 cost: 0.69331646 W= [[-0.05305941]\n",
      " [-0.04744943]] b = [0.05961003]\n",
      "step =  900 cost: 0.6932242 W= [[-0.03542079]\n",
      " [-0.03242382]] b = [0.04023699]\n",
      "step =  1000 cost: 0.69318223 W= [[-0.02369759]\n",
      " [-0.02209654]] b = [0.02715926]\n",
      "step =  1100 cost: 0.69316316 W= [[-0.01588257]\n",
      " [-0.01502727]] b = [0.01833173]\n",
      "step =  1200 cost: 0.69315445 W= [[-0.01066004]\n",
      " [-0.01020313]] b = [0.01237333]\n",
      "step =  1300 cost: 0.6931505 W= [[-0.00716301]\n",
      " [-0.00691892]] b = [0.00835158]\n",
      "step =  1400 cost: 0.6931487 W= [[-0.0048176]\n",
      " [-0.0046872]] b = [0.00563701]\n",
      "step =  1500 cost: 0.6931479 W= [[-0.00324254]\n",
      " [-0.00317288]] b = [0.0038048]\n",
      "step =  1600 cost: 0.69314754 W= [[-0.0021837 ]\n",
      " [-0.00214649]] b = [0.00256812]\n",
      "step =  1700 cost: 0.6931473 W= [[-0.00147132]\n",
      " [-0.00145143]] b = [0.0017334]\n",
      "step =  1800 cost: 0.69314724 W= [[-0.0009917 ]\n",
      " [-0.00098107]] b = [0.00117]\n",
      "step =  1900 cost: 0.6931472 W= [[-0.00066862]\n",
      " [-0.00066294]] b = [0.00078973]\n",
      "step =  2000 cost: 0.6931472 W= [[-0.00045091]\n",
      " [-0.00044787]] b = [0.00053304]\n",
      "step =  2100 cost: 0.6931472 W= [[-0.00030415]\n",
      " [-0.00030252]] b = [0.00035985]\n",
      "step =  2200 cost: 0.6931472 W= [[-0.00020518]\n",
      " [-0.00020432]] b = [0.00024287]\n",
      "step =  2300 cost: 0.6931472 W= [[-0.00013842]\n",
      " [-0.00013796]] b = [0.00016392]\n",
      "step =  2400 cost: 0.6931472 W= [[-9.3395123e-05]\n",
      " [-9.3147835e-05]] b = [0.00011064]\n",
      "step =  2500 cost: 0.6931472 W= [[-6.3013897e-05]\n",
      " [-6.2881365e-05]] b = [7.467921e-05]\n",
      "step =  2600 cost: 0.6931472 W= [[-4.2518834e-05]\n",
      " [-4.2450385e-05]] b = [5.040522e-05]\n",
      "step =  2700 cost: 0.6931472 W= [[-2.8690556e-05]\n",
      " [-2.8651917e-05]] b = [3.401543e-05]\n",
      "step =  2800 cost: 0.6931472 W= [[-1.9357216e-05]\n",
      " [-1.9333480e-05]] b = [2.2961e-05]\n",
      "step =  2900 cost: 0.6931472 W= [[-1.3059241e-05]\n",
      " [-1.3044444e-05]] b = [1.5496265e-05]\n",
      "step =  3000 cost: 0.6931472 W= [[-8.806448e-06]\n",
      " [-8.799101e-06]] b = [1.0459674e-05]\n",
      "step =  3100 cost: 0.6931472 W= [[-5.9409554e-06]\n",
      " [-5.9380782e-06]] b = [7.0562483e-06]\n",
      "step =  3200 cost: 0.6931472 W= [[-3.9978408e-06]\n",
      " [-3.9979436e-06]] b = [4.77935e-06]\n",
      "step =  3300 cost: 0.6931472 W= [[-2.6887751e-06]\n",
      " [-2.6888779e-06]] b = [3.2363325e-06]\n",
      "step =  3400 cost: 0.6931472 W= [[-1.8185451e-06]\n",
      " [-1.8186479e-06]] b = [2.1753692e-06]\n",
      "step =  3500 cost: 0.6931472 W= [[-1.2269685e-06]\n",
      " [-1.2270713e-06]] b = [1.4779963e-06]\n",
      "step =  3600 cost: 0.6931472 W= [[-8.4400619e-07]\n",
      " [-8.4410897e-07]] b = [9.65393e-07]\n",
      "step =  3700 cost: 0.6931472 W= [[-5.5715833e-07]\n",
      " [-5.5726110e-07]] b = [6.755664e-07]\n",
      "step =  3800 cost: 0.6931472 W= [[-3.6046256e-07]\n",
      " [-3.6056534e-07]] b = [4.7887227e-07]\n",
      "step =  3900 cost: 0.6931472 W= [[-2.4646891e-07]\n",
      " [-2.4657169e-07]] b = [3.3656633e-07]\n",
      "step =  4000 cost: 0.6931472 W= [[-1.7196297e-07]\n",
      " [-1.7206574e-07]] b = [2.6206166e-07]\n",
      "step =  4100 cost: 0.6931472 W= [[-1.1459328e-07]\n",
      " [-1.1469605e-07]] b = [1.7041961e-07]\n",
      "step =  4200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  4900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5100 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  5900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6100 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  6900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7100 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  7900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8100 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  8900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9100 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9200 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9300 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9400 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9500 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9600 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9700 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9800 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  9900 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "step =  10000 cost: 0.6931472 W= [[-8.8516146e-08]\n",
      " [-8.8618918e-08]] b = [1.19010416e-07]\n",
      "\n",
      "Hypothesis: [[0.5]\n",
      " [0.5]\n",
      " [0.5]\n",
      " [0.5]] \n",
      "Correct: [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "#w, b는 직접입력하는 값이 아니다.\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=np.float32)\n",
    "y_data = np.array([[0],[1],[1],[0]], dtype = np.float32)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2], name= 'x-input')\n",
    "Y = tf.placeholder(tf.float32, [None,1], name= 'y-input')\n",
    "\n",
    "w = tf.Variable(tf.random_normal([2,1]), name = 'weight')\n",
    "b = tf.Variable(tf.random_normal([1]), name = 'bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,w)+b)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate = 0.1).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype= tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "    \n",
    "sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        \n",
    "        if step %100 ==0:\n",
    "            print(\"step = \", step, \"cost:\", sess.run(cost, feed_dict={X:x_data, Y:y_data}),\n",
    "                 \"W=\", sess.run(w), \"b =\", sess.run(b))\n",
    "            \n",
    "#accuacy\n",
    "h, c, a = sess.run([hypothesis, predicted, accuracy],feed_dict={X:x_data, Y:y_data})\n",
    "print('\\nHypothesis:',h,\"\\nCorrect:\",c, \"\\nAccuracy:\",a)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치, 가중합, 바이어스, 활성화 함수\n",
    "- 가중합: 입력 값과 가중치의 곱을 모두 더한 다음 거기에 바이어스를 더한 값\n",
    "- 가중합의 결과를 놓고 1과 0을 출력해서 다음으로 보냄\n",
    "- 0과 1을 판단하는 함수가 있는데 이걸, 활성화 함수라고 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 값:(0, 0)출력 값:0\n",
      "입력 값:(1, 0)출력 값:1\n",
      "입력 값:(0, 1)출력 값:1\n",
      "입력 값:(1, 1)출력 값:1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "w11 = np.array([-7.40,7.40])\n",
    "w12 = np.array([8.67,8.67])\n",
    "w2 = np.array([7.41,7.41])\n",
    "\n",
    "b1 =11.28\n",
    "b2 = -3.87\n",
    "b3 = -11.29\n",
    "\n",
    "#퍼셉트론\n",
    "def MLP(x,w,b):\n",
    "    y = np.sum(w*x)+b\n",
    "    if y <=0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def NAND(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w11, b1)\n",
    "\n",
    "\n",
    "def OR(x1,x2):\n",
    "    return MLP(np.array([x1,x2]), w12, b2)\n",
    "\n",
    "\n",
    "def AND(x1, x2):\n",
    "    return MLP(np.array([x1,x2]), w2, b3)\n",
    "\n",
    "def XOR(x1,x2):\n",
    "    return AND(NAND(x1,x2), OR(x1,x2))\n",
    "\n",
    "if  __name__ == '__main__': #\n",
    "    for x in [(0,0),(1,0), (0,1), (1,1)]:\n",
    "        y = XOR(x[0], x[1])\n",
    "        print(\"입력 값:\" + str(x)+ \"출력 값:\"+str(y))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 오차 역전파의 개념\n",
    " - 최적화의 계산 방향이 출력층에서 시작해 앞으로 진행됨: 오차 역전파라고 부름\n",
    " - <구동방법>\n",
    " \n",
    " 1) 임의의 초기 가중치(w1)을 준 뒤 결과(y)를 계산한다.\n",
    " \n",
    " 2) 계산 결과와 우리가 원하는 값 사이의 오차를 구한다.\n",
    " \n",
    " 3) 경사 하강법을 이용해 바로 앞 가중치를 오차가 작아지는 방향으로 업데이트함.\n",
    " \n",
    " 4) 오차가 줄어들지 않을 때까지 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8542047 [[0.5364651]\n",
      " [1.6062909]]\n",
      "100 0.6940172 [[0.2534413]\n",
      " [1.1044824]]\n",
      "200 0.6937219 [[0.23457737]\n",
      " [1.1029341 ]]\n",
      "300 0.6934624 [[0.21850692]\n",
      " [1.1054965 ]]\n",
      "400 0.6932176 [[0.20342349]\n",
      " [1.1092877 ]]\n",
      "500 0.6929726 [[0.18905266]\n",
      " [1.1143419 ]]\n",
      "600 0.69271225 [[0.17514086]\n",
      " [1.1207901 ]]\n",
      "700 0.69241935 [[0.16143245]\n",
      " [1.1288502 ]]\n",
      "800 0.6920731 [[0.14765556]\n",
      " [1.1388422 ]]\n",
      "900 0.69164747 [[0.13350557]\n",
      " [1.1512102 ]]\n",
      "1000 0.6911092 [[0.11862805]\n",
      " [1.1665429 ]]\n",
      "1100 0.69041675 [[0.10259673]\n",
      " [1.1855905 ]]\n",
      "1200 0.689522 [[0.08489148]\n",
      " [1.209247  ]]\n",
      "1300 0.68837726 [[0.06488053]\n",
      " [1.238464  ]]\n",
      "1400 0.68695074 [[0.04181178]\n",
      " [1.274051  ]]\n",
      "1500 0.6852457 [[0.01483625]\n",
      " [1.3163553 ]]\n",
      "1600 0.6833044 [[-0.01692634]\n",
      " [ 1.3649862 ]]\n",
      "1700 0.6811763 [[-0.05427944]\n",
      " [ 1.4188704 ]]\n",
      "1800 0.67886794 [[-0.09784769]\n",
      " [ 1.4767892 ]]\n",
      "1900 0.6763216 [[-0.1480632]\n",
      " [ 1.5380341]]\n",
      "2000 0.6734357 [[-0.2052781]\n",
      " [ 1.6027037]]\n",
      "2100 0.6700915 [[-0.2699576]\n",
      " [ 1.6715416]]\n",
      "2200 0.6661624 [[-0.34288868]\n",
      " [ 1.745598  ]]\n",
      "2300 0.66150093 [[-0.4253758]\n",
      " [ 1.8259507]]\n",
      "2400 0.6559128 [[-0.5194611]\n",
      " [ 1.9135754]]\n",
      "2500 0.6491107 [[-0.62822443]\n",
      " [ 2.0093112 ]]\n",
      "2600 0.6406375 [[-0.7562097]\n",
      " [ 2.113885 ]]\n",
      "2700 0.6297498 [[-0.9099149]\n",
      " [ 2.2279665]]\n",
      "2800 0.61530304 [[-1.0979538]\n",
      " [ 2.3522708]]\n",
      "2900 0.59582704 [[-1.3298723]\n",
      " [ 2.4878604]]\n",
      "3000 0.5700054 [[-1.6126859]\n",
      " [ 2.6367362]]\n",
      "3100 0.5372715 [[-1.9467446]\n",
      " [ 2.8022153]]\n",
      "3200 0.49798033 [[-2.3245595]\n",
      " [ 2.9880347]]\n",
      "3300 0.4535164 [[-2.7327433]\n",
      " [ 3.1960518]]\n",
      "3400 0.40644634 [[-3.1546931]\n",
      " [ 3.4242003]]\n",
      "3500 0.35995996 [[-3.5737493]\n",
      " [ 3.6665359]]\n",
      "3600 0.31675845 [[-3.976424 ]\n",
      " [ 3.9154027]]\n",
      "3700 0.2783746 [[-4.354138 ]\n",
      " [ 4.1637774]]\n",
      "3800 0.24522993 [[-4.7029977]\n",
      " [ 4.406446 ]]\n",
      "3900 0.217044 [[-5.0225077]\n",
      " [ 4.640092 ]]\n",
      "4000 0.19322176 [[-5.3141413]\n",
      " [ 4.86292  ]]\n",
      "4100 0.17309459 [[-5.580312 ]\n",
      " [ 5.0741887]]\n",
      "4200 0.15603684 [[-5.823705]\n",
      " [ 5.273839]]\n",
      "4300 0.14150885 [[-6.046945]\n",
      " [ 5.462214]]\n",
      "4400 0.12906191 [[-6.252437]\n",
      " [ 5.639883]]\n",
      "4500 0.11833093 [[-6.442309 ]\n",
      " [ 5.8075223]]\n",
      "4600 0.10902068 [[-6.6184173]\n",
      " [ 5.965836 ]]\n",
      "4700 0.100892864 [[-6.7823596]\n",
      " [ 6.1155314]]\n",
      "4800 0.09375515 [[-6.935513 ]\n",
      " [ 6.2572784]]\n",
      "4900 0.08745137 [[-7.079059 ]\n",
      " [ 6.3917065]]\n",
      "5000 0.08185433 [[-7.214018 ]\n",
      " [ 6.5193987]]\n",
      "5100 0.07685988 [[-7.341266 ]\n",
      " [ 6.6408834]]\n",
      "5200 0.07238206 [[-7.461567]\n",
      " [ 6.756649]]\n",
      "5300 0.068349704 [[-7.5755825]\n",
      " [ 6.867133 ]]\n",
      "5400 0.06470343 [[-7.6838894]\n",
      " [ 6.972742 ]]\n",
      "5500 0.06139362 [[-7.786992 ]\n",
      " [ 7.0738297]]\n",
      "5600 0.05837827 [[-7.8853393]\n",
      " [ 7.1707263]]\n",
      "5700 0.055621758 [[-7.979323]\n",
      " [ 7.263731]]\n",
      "5800 0.053093936 [[-8.06929 ]\n",
      " [ 7.353113]]\n",
      "5900 0.050768737 [[-8.155557 ]\n",
      " [ 7.4391217]]\n",
      "6000 0.04862396 [[-8.238396]\n",
      " [ 7.521975]]\n",
      "6100 0.046640366 [[-8.318057 ]\n",
      " [ 7.6018786]]\n",
      "6200 0.044801205 [[-8.394766 ]\n",
      " [ 7.6790195]]\n",
      "6300 0.043091938 [[-8.46872 ]\n",
      " [ 7.753568]]\n",
      "6400 0.04149989 [[-8.540107 ]\n",
      " [ 7.8256774]]\n",
      "6500 0.04001374 [[-8.609091]\n",
      " [ 7.895497]]\n",
      "6600 0.03862387 [[-8.675815]\n",
      " [ 7.963154]]\n",
      "6700 0.037321478 [[-8.740423 ]\n",
      " [ 8.0287695]]\n",
      "6800 0.036098897 [[-8.803038]\n",
      " [ 8.092455]]\n",
      "6900 0.034949265 [[-8.863777]\n",
      " [ 8.154317]]\n",
      "7000 0.033866465 [[-8.922744]\n",
      " [ 8.214448]]\n",
      "7100 0.032845013 [[-8.980037]\n",
      " [ 8.272941]]\n",
      "7200 0.03188006 [[-9.035744]\n",
      " [ 8.329871]]\n",
      "7300 0.030967211 [[-9.089947]\n",
      " [ 8.385322]]\n",
      "7400 0.030102376 [[-9.142725 ]\n",
      " [ 8.4393635]]\n",
      "7500 0.029282141 [[-9.194147]\n",
      " [ 8.492057]]\n",
      "7600 0.028503157 [[-9.24428 ]\n",
      " [ 8.543469]]\n",
      "7700 0.027762529 [[-9.293183]\n",
      " [ 8.593657]]\n",
      "7800 0.027057527 [[-9.340918]\n",
      " [ 8.642675]]\n",
      "7900 0.0263857 [[-9.387534]\n",
      " [ 8.690576]]\n",
      "8000 0.025744926 [[-9.4330845]\n",
      " [ 8.737401 ]]\n",
      "8100 0.025133112 [[-9.4776125]\n",
      " [ 8.783202 ]]\n",
      "8200 0.024548357 [[-9.521161]\n",
      " [ 8.828018]]\n",
      "8300 0.023989003 [[-9.563774]\n",
      " [ 8.871891]]\n",
      "8400 0.023453454 [[-9.605491]\n",
      " [ 8.914856]]\n",
      "8500 0.022940218 [[-9.646344]\n",
      " [ 8.956949]]\n",
      "8600 0.022448014 [[-9.686372]\n",
      " [ 8.998204]]\n",
      "8700 0.021975629 [[-9.725601]\n",
      " [ 9.038651]]\n",
      "8800 0.021521885 [[-9.764067]\n",
      " [ 9.078322]]\n",
      "8900 0.021085773 [[-9.801794]\n",
      " [ 9.117244]]\n",
      "9000 0.02066621 [[-9.838815]\n",
      " [ 9.155443]]\n",
      "9100 0.02026238 [[-9.875152 ]\n",
      " [ 9.1929455]]\n",
      "9200 0.019873412 [[-9.910827]\n",
      " [ 9.229776]]\n",
      "9300 0.01949853 [[-9.945866]\n",
      " [ 9.265956]]\n",
      "9400 0.01913694 [[-9.98029 ]\n",
      " [ 9.301511]]\n",
      "9500 0.018788021 [[-10.014122]\n",
      " [  9.336456]]\n",
      "9600 0.018451165 [[-10.0473795]\n",
      " [  9.370814 ]]\n",
      "9700 0.018125724 [[-10.080082]\n",
      " [  9.404606]]\n",
      "9800 0.01781111 [[-10.112249]\n",
      " [  9.437845]]\n",
      "9900 0.017506886 [[-10.143893 ]\n",
      " [  9.4705515]]\n",
      "10000 0.017212452 [[-10.1750345]\n",
      " [  9.50274  ]]\n",
      "\n",
      "Hypothesis: [[0.01442865]\n",
      " [0.9845505 ]\n",
      " [0.98454535]\n",
      " [0.02290431]] \n",
      "Correct: [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "learning_rate = 0.1\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "x_data = [[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data = [[0],[1],[1],[0]]\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,2])\n",
    "Y = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([2,2]), name = 'weight1')\n",
    "b1 = tf.Variable(tf.random_normal([2]), name ='bias1')\n",
    "layer1 =tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "W2 = tf.Variable(tf.random_normal([2,1], name ='weight2'))\n",
    "b2 = tf.Variable(tf.random_normal([1]), name = 'bias2')\n",
    "hypothesis = tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+ (1-Y)*tf.log(1-hypothesis))\n",
    "train = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "predicted = tf.cast(hypothesis > 0.5, dtype = tf.float32)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        sess.run(train, feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 100 == 0:\n",
    "            print(step, sess.run(cost, feed_dict={X:x_data, Y:y_data}),sess.run(W2))\n",
    "            \n",
    "            \n",
    "    h,c,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "    print('\\nHypothesis:',h,\"\\nCorrect:\",c, \"\\nAccuracy:\",a)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
